{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 22:04:16.665425: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-10 22:04:16.675834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-10 22:04:16.690676: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-10 22:04:16.690714: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-10 22:04:16.700921: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-10 22:04:17.445665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda, Dropout, BatchNormalization, Add, GlobalAveragePooling2D, concatenate\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Function to load images from a folder\n",
    "# def load_images_from_folder(folder, augmentation=False):\n",
    "#     images = []\n",
    "#     datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1,\n",
    "#                                  shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest') if augmentation else None\n",
    "\n",
    "#     for filename in os.listdir(folder):\n",
    "#         img_path = os.path.join(folder, filename)\n",
    "#         img = preprocess_image(img_path)\n",
    "#         if img is not None:\n",
    "#             if augmentation:\n",
    "#                 img = np.expand_dims(img, axis=0)\n",
    "#                 for batch in datagen.flow(img, batch_size=1):\n",
    "#                     images.append(batch[0])\n",
    "#                     break\n",
    "#             else:\n",
    "#                 images.append(img)\n",
    "\n",
    "#     return np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image in grayscale\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(128, 128))\n",
    "    img = img_to_array(img).astype('uint8')\n",
    "    \n",
    "    # Apply GaussianBlur to reduce noise and improve edge detection\n",
    "    img = cv2.GaussianBlur(img, (13, 13), 0)\n",
    "    \n",
    "    # Apply adaptive thresholding to handle varying lighting conditions\n",
    "    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, 3)\n",
    "    \n",
    "    # Apply edge detection\n",
    "    edges = cv2.Canny(img, threshold1=75, threshold2=200)\n",
    "    \n",
    "    # Dilate the edges to broaden the lines\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "    \n",
    "    # Invert the edges: detected edges should be black, and the rest should be white\n",
    "    edges = cv2.bitwise_not(edges)\n",
    "    \n",
    "    # Normalize the image\n",
    "    edges = edges / 255.0\n",
    "    edges = np.expand_dims(edges, axis=-1)\n",
    "    \n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if img_path is not None:\n",
    "            processed_img = preprocess_image(img_path)\n",
    "            images.append(processed_img)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(base_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    for person_id in range(1, 65):  # Assuming you have 64 persons\n",
    "        if person_id in [5,7,8,10,11]:\n",
    "            continue\n",
    "        person_id = str(person_id).zfill(3)\n",
    "        real_folder = os.path.join(base_path, person_id)\n",
    "        forge_folder = os.path.join(base_path, f\"{person_id}_forg\")\n",
    "\n",
    "        real_images = load_images_from_folder(real_folder)\n",
    "        forge_images = load_images_from_folder(forge_folder)\n",
    "\n",
    "        for img in real_images:\n",
    "            X.append(img)\n",
    "            y.append(0)  # Label for genuine signatures\n",
    "\n",
    "        for img in forge_images:\n",
    "            X.append(img)\n",
    "            y.append(1)  # Label for forged signatures\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X / 255.0  # Normalize the images\n",
    "    X = X.reshape(-1, 128, 128, 1)  # Reshape for the CNN\n",
    "    return X, y\n",
    "\n",
    "base_path = 'sign_data/train'\n",
    "X, y = create_dataset(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display a single image before and after preprocessing\n",
    "def display_image_before_after(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (128, 128))\n",
    "    processed_img = preprocess_image(file_path)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Original Image')\n",
    "    plt.imshow(img_resized, cmap='gray')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Processed Image')\n",
    "    plt.imshow(processed_img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Example image display\n",
    "example_image_path = 'img4.jpeg'  # Provide a valid image path here\n",
    "display_image_before_after(example_image_path)\n",
    "# Example image display\n",
    "example_image_path = 'img1.jpeg'  # Provide a valid image path here\n",
    "display_image_before_after(example_image_path)\n",
    "# Example image display\n",
    "example_image_path = 'real.jpeg'  # Provide a valid image path here\n",
    "display_image_before_after(example_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Add, Input, Activation, concatenate, Reshape, multiply, Attention\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "def squeeze_excite_block(input, ratio=16):\n",
    "    init = input\n",
    "    channel_axis = -1\n",
    "    filters = init.shape[channel_axis]\n",
    "\n",
    "    se_shape = (1, 1, filters)\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    x = multiply([init, se])\n",
    "    return x\n",
    "\n",
    "def residual_block(x, filters, kernel_size=(3, 3), stride=1):\n",
    "    res = Conv2D(filters, (1, 1), strides=stride, padding='same')(x)\n",
    "    \n",
    "    x = DepthwiseConv2D(kernel_size, padding='same', strides=stride, activation='relu')(x)\n",
    "    x = Conv2D(filters, (1, 1), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Add()([x, res])\n",
    "    x = Activation('relu')(x)\n",
    "    x = squeeze_excite_block(x)\n",
    "    return x\n",
    "\n",
    "def build_enhanced_model(input_shape=(128, 128, 1)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Initial convolutional block\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # Residual blocks\n",
    "    x = residual_block(x, 128)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = residual_block(x, 256)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = residual_block(x, 512)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = residual_block(x, 1024)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = residual_block(x, 2048)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Global Average Pooling and final dense layers\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    optimizer = AdamW(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.1))\n",
    "\n",
    "model = build_enhanced_model()\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    LearningRateScheduler(lr_schedule),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=18, epochs=50, validation_data=(X_test, y_test), callbacks=callbacks)\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('enhanced_signature_verification_model3_1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "models=[]\n",
    "models.append(load_model(\"enhanced_signature_verification_model3.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image1(image_path):\n",
    "    processed_img = preprocess_image(image_path)\n",
    "    processed_img = processed_img / 255.0\n",
    "    processed_img = processed_img.reshape(1, 128, 128, 1)\n",
    "    return processed_img\n",
    "\n",
    "def predict_signature(model, real_signature_path, test_signature_path):\n",
    "    real_img = preprocess_image1(real_signature_path)\n",
    "    test_img = preprocess_image1(test_signature_path)\n",
    "    \n",
    "    real_pred = model.predict(real_img)\n",
    "    test_pred = model.predict(test_img)\n",
    "    \n",
    "    difference = np.abs(real_pred - test_pred)\n",
    "    return difference*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "real_signature_path = 'img1.jpeg'  # Provide a valid image path\n",
    "test_signature_path = 'img2.jpeg'  # Provide a valid image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Difference: [[6.278434]] by model1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,model in enumerate(models):\n",
    "    difference = predict_signature(model, real_signature_path, test_signature_path)\n",
    "    print(f\"Difference: {difference} by model{i+1}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "real_signature_path = 'img1.jpeg'  # Provide a valid image path\n",
    "test_signature_path = 'img3.jpeg'  # Provide a valid image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Difference: [[0.00011987]] by model1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,model in enumerate(models):\n",
    "    difference = predict_signature(model, real_signature_path, test_signature_path)\n",
    "    print(f\"Difference: {difference} by model{i+1}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "real_signature_path = 'img1.jpeg'   # Provide a valid image path\n",
    "test_signature_path = 'img4.jpeg'  # Provide a valid image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Difference: [[98.91398]] by model1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,model in enumerate(models):\n",
    "    difference = predict_signature(model, real_signature_path, test_signature_path)\n",
    "    print(f\"Difference: {difference} by model{i+1}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "real_signature_path = 'real.jpeg'   # Provide a valid image path\n",
    "test_signature_path = 'img1.jpeg'  # Provide a valid image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Difference: [[6.278434]] by model1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,model in enumerate(models):\n",
    "    difference = predict_signature(model, real_signature_path, test_signature_path)\n",
    "    print(f\"Difference: {difference} by model{i+1}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
