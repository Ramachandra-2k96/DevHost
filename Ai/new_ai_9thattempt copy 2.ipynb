{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signature Validation AI Development Outline\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Collection and Preparation\n",
    "\n",
    "- **Data Acquisition:**\n",
    "  - Obtain a dataset of signatures with labels (genuine or forged).\n",
    "\n",
    "- **Data Preprocessing:**\n",
    "  - Convert signature images into a suitable format (e.g., grayscale, standardized size).\n",
    "  - Split the dataset into training and testing sets.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feature Extraction\n",
    "\n",
    "- **Image Processing Techniques:**\n",
    "  - Extract features using methods like HOG, SIFT, or CNNs.\n",
    "  - Normalize and preprocess extracted features.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Model Selection and Training\n",
    "\n",
    "- **Choose a Model:**\n",
    "  - Select CNNs.\n",
    "\n",
    "- **Training:**\n",
    "  - Train the selected model on the preprocessed training dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Model Evaluation\n",
    "\n",
    "- **Testing:**\n",
    "  - Evaluate the model's accuracy using the testing dataset.\n",
    "  - Use metrics like accuracy.\n",
    "\n",
    "- **Fine-tuning:**\n",
    "  - Adjust the model based on evaluation results.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Add, Input, Activation, Reshape, multiply, GlobalMaxPooling2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start preprosessing the image so that the model can understand the image better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Load the image in grayscale and resize it to (128, 128)\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(128, 128))\n",
    "    \n",
    "    # Convert the image to a numpy array of type 'uint8'\n",
    "    img = img_to_array(img).astype('uint8')\n",
    "    \n",
    "    # Apply Gaussian Blur to the image to reduce noise\n",
    "    blurred = cv2.GaussianBlur(img, (3, 3), 1)\n",
    "    \n",
    "    # Apply Canny edge detection to detect edges in the image\n",
    "    edges = cv2.Canny(blurred, threshold1=70, threshold2=100)\n",
    "    \n",
    "    # Dilate the edges to thicken the detected edges\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    edges = cv2.dilate(edges, kernel, iterations=2)\n",
    "    \n",
    "    # Invert the edges: detected edges should be black, and the rest should be white\n",
    "    edges = cv2.bitwise_not(edges)\n",
    "    \n",
    "    # Normalize the image pixel values to be between 0 and 1\n",
    "    edges = edges / 255.0\n",
    "    \n",
    "    # Add a channel dimension to the image\n",
    "    edges = np.expand_dims(edges, axis=-1)\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Steps:\n",
    "\n",
    "1. **Load and Resize Image:**\n",
    "   - `img = load_img(image_path, color_mode='grayscale', target_size=(128, 128))`\n",
    "     - Loads the image from `image_path` in grayscale mode and resizes it to a fixed size of 128x128 pixels.\n",
    "\n",
    "2. **Convert to Numpy Array:**\n",
    "   - `img = img_to_array(img).astype('uint8')`\n",
    "     - Converts the image to a numpy array of type `uint8`, which is suitable for processing with OpenCV.\n",
    "\n",
    "3. **Apply Gaussian Blur:**\n",
    "   - `blurred = cv2.GaussianBlur(img, (3, 3), 1)`\n",
    "     - Applies Gaussian blur with a kernel size of (3, 3) and a sigma of 1 to reduce noise in the image.\n",
    "\n",
    "4. **Edge Detection (Canny):**\n",
    "   - `edges = cv2.Canny(blurred, threshold1=70, threshold2=100)`\n",
    "     - Detects edges using the Canny edge detection algorithm with thresholds set at 70 and 100.\n",
    "\n",
    "5. **Dilate Edges:**\n",
    "   - `edges = cv2.dilate(edges, kernel, iterations=2)`\n",
    "     - Dilates the detected edges using a 2x2 kernel to thicken them for better feature extraction.\n",
    "\n",
    "6. **Invert Edges:**\n",
    "   - `edges = cv2.bitwise_not(edges)`\n",
    "     - Inverts the edges so that detected edges are black and the background is white, which is often preferred for neural network input.\n",
    "\n",
    "7. **Normalize Image:**\n",
    "   - `edges = edges / 255.0`\n",
    "     - Normalizes the pixel values of the image to be between 0 and 1, which is typical for neural network inputs.\n",
    "\n",
    "8. **Expand Dimension:**\n",
    "   - `edges = np.expand_dims(edges, axis=-1)`\n",
    "     - Adds a channel dimension to the image to make it suitable for feeding into a convolutional neural network (CNN), where `-1` signifies the last axis.\n",
    "\n",
    "This preprocessing function prepares the image for further feature extraction or directly as input to a CNN model for signature verification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []  # Initialize an empty list to store processed images\n",
    "    for filename in tqdm(os.listdir(folder)):  # Iterate through each file in the folder\n",
    "        img_path = os.path.join(folder, filename)  # Construct the full path to the image file\n",
    "        if os.path.isfile(img_path):  # Check if the path leads to a file (not a directory)\n",
    "            processed_img = preprocess_image(img_path)  # Preprocess the image using a predefined function\n",
    "            images.append(processed_img)  # Append the processed image to the list of images\n",
    "    return np.array(images)  # Convert the list of processed images into a numpy array and return itz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation and Improvements:\n",
    "\n",
    "1. **Iterate through Files in Folder:**\n",
    "   - `for filename in tqdm(os.listdir(folder)):`  \n",
    "     - Iterates through each file in the `folder` directory using `os.listdir()`. The `tqdm` wrapper is used to display a progress bar, which is helpful when processing a large number of images.\n",
    "\n",
    "2. **Check if File Exists:**\n",
    "   - `if os.path.isfile(img_path):`\n",
    "     - Ensures that `img_path` points to a file and not a directory. This check prevents errors that could occur if `filename` refers to a subdirectory or invalid path.\n",
    "\n",
    "3. **Preprocess and Append Images:**\n",
    "   - `processed_img = preprocess_image(img_path)`\n",
    "     - Calls the `preprocess_image` function defined earlier to preprocess each image before appending it to the `images` list.\n",
    "\n",
    "4. **Convert to Numpy Array:**\n",
    "   - `return np.array(images)`\n",
    "     - Converts the list of processed images into a numpy array before returning it. This conversion is typically necessary for compatibility with neural network frameworks like Keras or TensorFlow.\n",
    "\n",
    "### Notes:\n",
    "- Make sure `preprocess_image` function is defined before `load_images_from_folder` and correctly handles image preprocessing as per your requirements.\n",
    "- Consider handling exceptions that may arise during image loading or preprocessing to ensure robustness in real-world scenarios.\n",
    "- Adjust the function according to specific needs, such as adding error handling, logging, or modifying preprocessing steps.\n",
    "\n",
    "This function will load and preprocess images from a specified folder, making them ready for further use in machine learning models or other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(base_path):\n",
    "    X = []  # Initialize an empty list to store images\n",
    "    y = []  # Initialize an empty list to store labels\n",
    "    for person_id in range(1, 65):  # Assuming there are 64 persons in total\n",
    "        if person_id in [5, 7, 8, 10, 11]:  # Skip specific person IDs\n",
    "            continue\n",
    "        \n",
    "        person_id_str = str(person_id).zfill(3)  # Format person_id as 3-digit string (e.g., '001')\n",
    "        \n",
    "        real_folder = os.path.join(base_path, person_id_str)  # Path to genuine signatures folder\n",
    "        forge_folder = os.path.join(base_path, f\"{person_id_str}_forg\")  # Path to forged signatures folder\n",
    "\n",
    "        real_images = load_images_from_folder(real_folder)  # Load genuine signature images\n",
    "        forge_images = load_images_from_folder(forge_folder)  # Load forged signature images\n",
    "\n",
    "        for img in real_images:\n",
    "            X.append(img)\n",
    "            y.append(0)  # Label 0 for genuine signatures\n",
    "\n",
    "        for img in forge_images:\n",
    "            X.append(img)\n",
    "            y.append(1)  # Label 1 for forged signatures\n",
    "\n",
    "    X = np.array(X)  # Convert list of images to numpy array\n",
    "    y = np.array(y)  # Convert list of labels to numpy array\n",
    "    \n",
    "    X = X / 255.0  # Normalize pixel values to [0, 1]\n",
    "    X = X.reshape(-1, 128, 128, 1)  # Reshape images for CNN input (assuming 128x128 grayscale images)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "base_path = 'sign_data/train'  # Base path where genuine and forged signature folders are located\n",
    "X, y = create_dataset(base_path)  # Create the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation with Comments:\n",
    "\n",
    "1. **Initialize Lists for Images and Labels:**\n",
    "   - `X = []` and `y = []`\n",
    "     - Initialize empty lists `X` to store images and `y` to store corresponding labels.\n",
    "\n",
    "2. **Iterate through Person IDs:**\n",
    "   - `for person_id in range(1, 65):`\n",
    "     - Loop through each person ID from 1 to 64, assuming each person has a folder of genuine signatures (`person_id`) and a folder of forged signatures (`person_id_forg`).\n",
    "\n",
    "3. **Skip Specific Person IDs:**\n",
    "   - `if person_id in [5, 7, 8, 10, 11]: continue`\n",
    "     - Skip specific person IDs (5, 7, 8, 10, 11) as indicated in the list.\n",
    "\n",
    "4. **Format Person ID:**\n",
    "   - `person_id_str = str(person_id).zfill(3)`\n",
    "     - Format the person ID as a 3-digit string (e.g., '001', '002') using `.zfill(3)`.\n",
    "\n",
    "5. **Construct Paths to Signature Folders:**\n",
    "   - `real_folder = os.path.join(base_path, person_id_str)`\n",
    "     - Construct the path to the folder containing genuine signatures for the current person.\n",
    "   - `forge_folder = os.path.join(base_path, f\"{person_id_str}_forg\")`\n",
    "     - Construct the path to the folder containing forged signatures for the current person.\n",
    "\n",
    "6. **Load Images from Folders:**\n",
    "   - `real_images = load_images_from_folder(real_folder)`\n",
    "     - Load genuine signature images from `real_folder` using the `load_images_from_folder` function.\n",
    "   - `forge_images = load_images_from_folder(forge_folder)`\n",
    "     - Load forged signature images from `forge_folder` using the `load_images_from_folder` function.\n",
    "\n",
    "7. **Label Assignment:**\n",
    "   - For each image loaded:\n",
    "     - Append the image (`img`) to `X`.\n",
    "     - Append the corresponding label (`0` for genuine, `1` for forged) to `y`.\n",
    "\n",
    "8. **Convert to Numpy Arrays:**\n",
    "   - `X = np.array(X)` and `y = np.array(y)`\n",
    "     - Convert lists `X` and `y` into numpy arrays for efficient handling in machine learning frameworks.\n",
    "\n",
    "9. **Normalization and Reshaping:**\n",
    "   - `X = X / 255.0`\n",
    "     - Normalize pixel values of images to the range [0, 1].\n",
    "   - `X = X.reshape(-1, 128, 128, 1)`\n",
    "     - Reshape the images for CNN input. Assuming images are grayscale and resized to 128x128 pixels, `-1` indicates that the first dimension (number of samples) is inferred based on the other dimensions.\n",
    "\n",
    "10. **Return Dataset:**\n",
    "    - `return X, y`\n",
    "      - Return the preprocessed images (`X`) and corresponding labels (`y`) as the final dataset.\n",
    "\n",
    "### Additional Notes:\n",
    "\n",
    "- **Progress Bar (tqdm):** The `tqdm` library is used optionally to provide a progress bar during the image loading process, which can be helpful for monitoring progress, especially with a large number of images.\n",
    "\n",
    "- **Error Handling:** Consider adding error handling within the `create_dataset` function or within the `load_images_from_folder` function to manage potential errors that may occur during image loading or preprocessing.\n",
    "\n",
    "- **Adjustments:** Depending on specific requirements (e.g., different image sizes, additional preprocessing steps), you may need to modify the function accordingly.\n",
    "\n",
    "This function is designed to create a dataset suitable for training a machine learning model for signature verification, combining genuine and forged signature images with appropriate labels. Adjust paths and parameters as per your dataset structure and project needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Display some sample images to see how your image preprosessing works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display a single image before and after preprocessing\n",
    "def display_image_before_after(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (128, 128))\n",
    "    processed_img = preprocess_image(file_path)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Original Image')\n",
    "    plt.imshow(img_resized, cmap='gray')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Processed Image')\n",
    "    plt.imshow(processed_img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Example image display\n",
    "example_image_path = 'WhatsApp Image 2024-07-10 at 10.49.40 PM.jpeg'  # Provide a valid image path here\n",
    "display_image_before_after(example_image_path)\n",
    "# Example image display\n",
    "example_image_path = 'img1.jpeg'  # Provide a valid image path here\n",
    "display_image_before_after(example_image_path)\n",
    "# Example image display\n",
    "example_image_path = 'sign_data/test/049/08_049.png'  # Provide a valid image path here\n",
    "display_image_before_after(example_image_path)# Example image display\n",
    "example_image_path = 'sign_data/train/001_forg/0201001_01.png'  # Provide a valid image path here\n",
    "display_image_before_after(example_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the Actual AI development Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_excite_block(input, ratio=16):\n",
    "    init = input\n",
    "    channel_axis = -1\n",
    "    filters = init.shape[channel_axis]\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape((1, 1, filters))(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    x = multiply([init, se])\n",
    "    return x\n",
    "\n",
    "def depthwise_separable_conv_block(x, filters, kernel_size=(3, 3), stride=1):\n",
    "    x = DepthwiseConv2D(kernel_size, padding='same', strides=stride, activation='relu')(x)\n",
    "    x = Conv2D(filters, (1, 1), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = squeeze_excite_block(x)\n",
    "    return x\n",
    "\n",
    "def build_smaller_model(input_shape=(128, 128, 1)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Initial convolutional block\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # Depthwise separable convolution blocks\n",
    "    x = depthwise_separable_conv_block(x, 64)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = depthwise_separable_conv_block(x, 128)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = depthwise_separable_conv_block(x, 256)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = depthwise_separable_conv_block(x, 512)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = depthwise_separable_conv_block(x, 1024)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Global Average Pooling and final dense layers\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    optimizer = AdamW(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.1))\n",
    "\n",
    "model = build_smaller_model()\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    LearningRateScheduler(lr_schedule),\n",
    "]\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation with Comments:\n",
    "\n",
    "1. **Squeeze and Excite Block (`squeeze_excite_block`):**\n",
    "   - Implements a squeeze and excite block, which enhances feature representation by focusing on important features and suppressing less relevant ones.\n",
    "   - `GlobalAveragePooling2D()` computes the average value for each channel across the spatial dimensions.\n",
    "   - `Reshape((1, 1, filters))(se)` reshapes the pooled features to prepare them for scaling.\n",
    "   - Two `Dense` layers with `relu` and `sigmoid` activations respectively adjust the importance of features across channels.\n",
    "\n",
    "2. **Depthwise Separable Convolution Block (`depthwise_separable_conv_block`):**\n",
    "   - Utilizes depthwise separable convolution, which reduces computational cost and model size while maintaining performance.\n",
    "   - `DepthwiseConv2D` performs convolution independently over each channel of input, followed by a `Conv2D` for combining outputs.\n",
    "   - `BatchNormalization` ensures stable training by normalizing inputs to each layer.\n",
    "   - `squeeze_excite_block` enhances the feature representation learned by convolution layers.\n",
    "\n",
    "3. **Model Architecture (`build_smaller_model`):**\n",
    "   - Begins with a standard convolutional layer followed by batch normalization and max pooling.\n",
    "   - Sequentially applies multiple depthwise separable convolution blocks, each followed by max pooling to downsample feature maps.\n",
    "   - Concludes with global average pooling to reduce spatial dimensions while maintaining channel information.\n",
    "   - Includes dropout regularization to prevent overfitting and a final dense layer with sigmoid activation for binary classification (genuine vs. forged signatures).\n",
    "\n",
    "4. **Optimizer and Compilation:**\n",
    "   - Uses `AdamW` optimizer with a specified learning rate (`0.0001`).\n",
    "   - Compiles the model with binary cross-entropy loss (suitable for binary classification tasks) and accuracy metric.\n",
    "\n",
    "5. **Learning Rate Scheduler (`lr_schedule`):**\n",
    "   - Adjusts the learning rate during training epochs.\n",
    "   - Maintains the initial learning rate for the first 10 epochs and decays it exponentially thereafter.\n",
    "\n",
    "6. **Callbacks:**\n",
    "   - `LearningRateScheduler` is a callback that adjusts the learning rate according to the defined `lr_schedule` function during training.\n",
    "\n",
    "7. **Model Summary:**\n",
    "   - `model.summary()` prints a concise summary of the model architecture, displaying layer types, output shapes, and number of parameters.\n",
    "\n",
    "### Additional Considerations:\n",
    "\n",
    "- **Data Input:** Ensure that input data (`X`) matches the expected shape (`input_shape`) defined in `build_smaller_model`.\n",
    "- **Training and Evaluation:** Implement the training loop using appropriate training data (`X_train, y_train`) and evaluation metrics to monitor model performance.\n",
    "- **Hyperparameters:** Fine-tune hyperparameters such as learning rate, dropout rate, and model depth based on validation performance.\n",
    "- **Deployment:** Consider requirements for deployment, such as model serialization (`model.save`) and inference implementation (`model.predict`).\n",
    "\n",
    "This structured approach to building and training the model ensures scalability, efficiency, and effectiveness in solving the signature verification task. Adjust the architecture and parameters based on specific data characteristics to achieve optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test), callbacks=callbacks)\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "1. **Training (`model.fit`):**\n",
    "   - `model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test), callbacks=callbacks)`\n",
    "     - Trains the model on training data (`X_train`, `y_train`) for 50 epochs with a batch size of 32.\n",
    "     - Uses validation data (`X_test`, `y_test`) to evaluate the model's performance after each epoch.\n",
    "     - `callbacks` include the `LearningRateScheduler` defined earlier, which adjusts the learning rate during training epochs.\n",
    "\n",
    "2. **Evaluation (`model.evaluate`):**\n",
    "   - `test_loss, test_acc = model.evaluate(X_test, y_test)`\n",
    "     - Evaluates the trained model on the test data (`X_test`, `y_test`).\n",
    "     - Computes and returns the test loss (`test_loss`) and test accuracy (`test_acc`).\n",
    "\n",
    "3. **Printing Test Accuracy:**\n",
    "   - `print(f\"Test Accuracy: {test_acc}\")`\n",
    "     - Prints the test accuracy after evaluating the model on the test set.\n",
    "\n",
    "### Additional Considerations:\n",
    "\n",
    "- **Data Preparation:** Ensure `X_train`, `y_train`, `X_test`, and `y_test` are properly prepared and normalized as required by the model.\n",
    "  \n",
    "- **Callbacks:** Adjust callbacks (`LearningRateScheduler`, etc.) based on training requirements and model performance.\n",
    "\n",
    "- **Hyperparameters:** Fine-tune hyperparameters such as batch size, epochs, learning rate, and model architecture based on validation results to achieve optimal performance.\n",
    "\n",
    "- **Monitoring Training Progress:** Utilize `history` object returned by `model.fit` to visualize training metrics (e.g., loss, accuracy) over epochs for both training and validation sets.\n",
    "\n",
    "- **Model Saving:** After training, consider saving the trained model (`model.save`) for future deployment or further analysis.\n",
    "\n",
    "By following these steps and considerations, you can effectively train and evaluate your signature verification model, ensuring robustness and accuracy in detecting genuine and forged signatures. Adjustments and optimizations can be made iteratively based on validation performance and specific project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Development is done. To Use the model in future we use this method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('enhanced_signature_verification_model3_4.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep in mind that there are several formate you can save the model but here the best way isto use the .keras formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the Actual job begins\n",
    "\n",
    "#### the task is to take real world data and test the models performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "models=[]\n",
    "models.append(load_model(\"enhanced_signature_verification_model3_4.keras\"))### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_image1(image_path):\n",
    "    # Preprocess the image using existing preprocess_image function\n",
    "    processed_img = preprocess_image(image_path)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    processed_img = processed_img / 255.0\n",
    "    \n",
    "    # Reshape the image to match model input shape\n",
    "    processed_img = processed_img.reshape(1, 128, 128, 1)\n",
    "    \n",
    "    return processed_img\n",
    "\n",
    "def predict_signature(model, real_signature_path, test_signature_path):\n",
    "    # Preprocess real and test signatures\n",
    "    real_img = preprocess_image1(real_signature_path)\n",
    "    test_img = preprocess_image1(test_signature_path)\n",
    "    \n",
    "    # Predict probabilities for real and test signatures\n",
    "    real_pred = model.predict(real_img)\n",
    "    test_pred = model.predict(test_img)\n",
    "    \n",
    "    # Calculate absolute difference in predictions\n",
    "    difference = np.abs(real_pred - test_pred)\n",
    "    \n",
    "    # Scale the difference for better interpretation\n",
    "    difference_scaled = difference * 100\n",
    "    \n",
    "    return difference_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "real_signature_path = 'img1.jpeg'  # Provide a valid image path of real image\n",
    "test_signature_path = 'img2.jpeg'  # Provide a valid image path of test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,model in enumerate(models):\n",
    "    difference = predict_signature(model, real_signature_path, test_signature_path) # use the model to get the test result\n",
    "    print(f\"Difference: {difference} by model{i+1}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Development and testing are complete! You can now integrate it into real-world websites and apps for signature testing.\n",
    "# Happy coding! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
