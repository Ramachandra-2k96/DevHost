{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 22:01:46.237750: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-10 22:01:46.323439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-10 22:01:46.440350: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-10 22:01:46.443059: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-10 22:01:46.538843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-10 22:01:47.577897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load images from a folder\n",
    "def load_images_from_folder(folder, augmentation=False):\n",
    "    images = []\n",
    "    datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1,\n",
    "                                 shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest') if augmentation else None\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = load_img(os.path.join(folder, filename), color_mode='grayscale', target_size=(155, 220))\n",
    "        if img is not None:\n",
    "            img = img_to_array(img)\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            if augmentation:\n",
    "                for batch in datagen.flow(img, batch_size=1):\n",
    "                    images.append(batch[0])\n",
    "                    break\n",
    "            else:\n",
    "                images.append(img[0])\n",
    "\n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load datasets\n",
    "def load_datasets(base_path, augmentation=False):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in range(1, 70):\n",
    "        if i in [5,7,8,10,11]:\n",
    "            continue\n",
    "        real_folder = os.path.join(base_path, f'{i:03}')\n",
    "        forge_folder = os.path.join(base_path, f'{i:03}_forg')\n",
    "        \n",
    "        real_images = load_images_from_folder(real_folder, augmentation)\n",
    "        forge_images = load_images_from_folder(forge_folder, augmentation)\n",
    "        \n",
    "        images = np.concatenate((real_images, forge_images), axis=0)\n",
    "        labels = np.array([1] * len(real_images) + [0] * len(forge_images))\n",
    "        \n",
    "        all_images.append(images)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    all_images = np.concatenate(all_images, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    return all_images, all_labels\n",
    "\n",
    "# Function to load datasets\n",
    "def load_datasets_test(base_path, augmentation=False):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in range(49, 70):\n",
    "        real_folder = os.path.join(base_path, f'{i:03}')\n",
    "        forge_folder = os.path.join(base_path, f'{i:03}_forg')\n",
    "        \n",
    "        real_images = load_images_from_folder(real_folder, augmentation)\n",
    "        forge_images = load_images_from_folder(forge_folder, augmentation)\n",
    "        \n",
    "        images = np.concatenate((real_images, forge_images), axis=0)\n",
    "        labels = np.array([1] * len(real_images) + [0] * len(forge_images))\n",
    "        \n",
    "        all_images.append(images)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    all_images = np.concatenate(all_images, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    return all_images, all_labels\n",
    "# Base path to the datasets\n",
    "train_path = 'sign_data/train'\n",
    "test_path = 'sign_data/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and normalize datasets\n",
    "train_images, train_labels = load_datasets(train_path, augmentation=True)\n",
    "test_images, test_labels = load_datasets_test(test_path, augmentation=False)\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to build the Signet model with enhancements\n",
    "def build_signet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (11, 11), strides=(4, 4), activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Conv2D(128, (5, 5), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "input_shape = (155, 220, 1)\n",
    "sig_model = build_signet_model(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom callback for live plotting\n",
    "class PlotLosses(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.fig, self.ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('accuracy'))\n",
    "        self.val_acc.append(logs.get('val_accuracy'))\n",
    "        self.i += 1\n",
    "\n",
    "        self.ax[0].clear()\n",
    "        self.ax[0].plot(self.x, self.losses, label=\"loss\")\n",
    "        self.ax[0].plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        self.ax[0].legend()\n",
    "        self.ax[0].set_title('Loss')\n",
    "\n",
    "        self.ax[1].clear()\n",
    "        self.ax[1].plot(self.x, self.acc, label=\"accuracy\")\n",
    "        self.ax[1].plot(self.x, self.val_acc, label=\"val_accuracy\")\n",
    "        self.ax[1].legend()\n",
    "        self.ax[1].set_title('Accuracy')\n",
    "\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for the Lambda layer\n",
    "def custom_abs_diff(tensors):\n",
    "    x, y = tensors\n",
    "    return tf.abs(x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Siamese model\n",
    "def siamese_model(base_model, input_shape):\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    processed_a = base_model(input_a)\n",
    "    processed_b = base_model(input_b)\n",
    "    L1_layer = Lambda(custom_abs_diff)([processed_a, processed_b])\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_layer)\n",
    "    model = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "    return model\n",
    "\n",
    "siamese_net = siamese_model(sig_model, input_shape)\n",
    "siamese_net.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Function to generate pairs of images for training\n",
    "def make_pairs(images, labels):\n",
    "    pair_images = []\n",
    "    pair_labels = []\n",
    "    num_classes = len(np.unique(labels))\n",
    "    idx = [np.where(labels == i)[0] for i in range(num_classes)]\n",
    "    for idxA in range(len(images)):\n",
    "        current_image = images[idxA]\n",
    "        label = labels[idxA]\n",
    "        # Positive Pair\n",
    "        idxB = np.random.choice(idx[label])\n",
    "        pos_image = images[idxB]\n",
    "        pair_images.append([current_image, pos_image])\n",
    "        pair_labels.append(1)\n",
    "        # Negative Pair\n",
    "        neg_label = np.random.choice(list(set(range(num_classes)) - set([label])))\n",
    "        idxB = np.random.choice(idx[neg_label])\n",
    "        neg_image = images[idxB]\n",
    "        pair_images.append([current_image, neg_image])\n",
    "        pair_labels.append(0)\n",
    "    return np.array(pair_images), np.array(pair_labels)\n",
    "\n",
    "pairs_train, labels_train = make_pairs(X_train, y_train)\n",
    "pairs_val, y_val = make_pairs(X_val, y_val)\n",
    "pairs_test, labels_test = make_pairs(test_images, test_labels)\n",
    "\n",
    "# Enhanced training with callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00003)\n",
    "plot_losses = PlotLosses()\n",
    "history = siamese_net.fit(\n",
    "    [pairs_train[:, 0], pairs_train[:, 1]], labels_train, \n",
    "    validation_data=([pairs_val[:, 0], pairs_val[:, 1]], y_val), \n",
    "    batch_size=8, epochs=50, \n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net.save('signet_model_augmented_b8.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(155, 220))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# Ensure custom Lambda function is registered during model loading\n",
    "custom_objects = {'custom_abs_diff': custom_abs_diff}\n",
    "\n",
    "# Load the model with custom objects\n",
    "model = load_model('signet_model_augmented.keras', custom_objects=custom_objects, compile=False)\n",
    "\n",
    "# Function to evaluate the genuineness of two signatures\n",
    "def evaluate_signature(img1_path, img2_path):\n",
    "    img1 = preprocess_image(img1_path)\n",
    "    img2 = preprocess_image(img2_path)\n",
    "    prediction = model.predict([img1, img2])\n",
    "    return prediction[0][0] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "fake_img_path = 'img2.jpeg'\n",
    "real_img_path = 'img4.jpeg'\n",
    "genuineness = evaluate_signature(fake_img_path, real_img_path)\n",
    "\n",
    "if genuineness > 75:\n",
    "    print(f\"genuine: {genuineness:.2f}%\")\n",
    "else:\n",
    "    print(f\"fraud: {genuineness:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
