{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 00:42:54.119046: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-29 00:42:54.377908: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-29 00:42:55.774494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda, Dropout, BatchNormalization, Add, GlobalAveragePooling2D, concatenate\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load images from a folder\n",
    "def load_images_from_folder(folder, augmentation=False):\n",
    "    images = []\n",
    "    datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1,\n",
    "                                 shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest') if augmentation else None\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = preprocess_image(img_path)\n",
    "        if img is not None:\n",
    "            if augmentation:\n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                for batch in datagen.flow(img, batch_size=1):\n",
    "                    images.append(batch[0])\n",
    "                    break\n",
    "            else:\n",
    "                images.append(img)\n",
    "\n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image in grayscale\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(128, 128))\n",
    "    img = img_to_array(img).astype('uint8')\n",
    "    \n",
    "    # Apply edge detection\n",
    "    edges = cv2.Canny(img, threshold1=30, threshold2=100)\n",
    "    \n",
    "    # Dilate the edges to broaden the lines\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "    \n",
    "    # Invert the edges: detected edges should be black, and the rest should be white\n",
    "    edges = cv2.bitwise_not(edges)\n",
    "    \n",
    "    # Normalize the image\n",
    "    edges = edges / 255.0\n",
    "    edges = np.expand_dims(edges, axis=-1)\n",
    "    \n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if img_path is not None:\n",
    "            processed_img = preprocess_image(img_path)\n",
    "            images.append(processed_img)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(base_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    for person_id in range(1, 65):  # Assuming you have 64 persons\n",
    "        if person_id in [5,7,8,10,11]:\n",
    "            continue\n",
    "        person_id = str(person_id).zfill(3)\n",
    "        real_folder = os.path.join(base_path, person_id)\n",
    "        forge_folder = os.path.join(base_path, f\"{person_id}_forg\")\n",
    "\n",
    "        real_images = load_images_from_folder(real_folder)\n",
    "        forge_images = load_images_from_folder(forge_folder)\n",
    "\n",
    "        for img in real_images:\n",
    "            X.append(img)\n",
    "            y.append(0)  # Label for genuine signatures\n",
    "\n",
    "        for img in forge_images:\n",
    "            X.append(img)\n",
    "            y.append(1)  # Label for forged signatures\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X / 255.0  # Normalize the images\n",
    "    X = X.reshape(-1, 128, 128, 1)  # Reshape for the CNN\n",
    "    return X, y\n",
    "\n",
    "base_path = 'sign_data/train'\n",
    "X, y = create_dataset(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Example image display\u001b[39;00m\n\u001b[1;32m     18\u001b[0m example_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Provide a valid image path here\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdisplay_image_before_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_image_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mdisplay_image_before_after\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(file_path, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[1;32m      5\u001b[0m img_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m processed_img \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m(file_path)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_image' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# Display a single image before and after preprocessing\n",
    "def display_image_before_after(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (128, 128))\n",
    "    processed_img = preprocess_image(file_path)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Original Image')\n",
    "    plt.imshow(img_resized, cmap='gray')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Processed Image')\n",
    "    plt.imshow(processed_img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Example image display\n",
    "example_image_path = 'real.jpeg'  # Provide a valid image path here\n",
    "display_image_before_after(example_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramachandra/.local/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-06-29 00:43:01.623568: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-29 00:43:01.715727: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 712ms/step - accuracy: 0.5470 - loss: 1.8664 - val_accuracy: 0.5081 - val_loss: 0.7144\n",
      "Epoch 2/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 716ms/step - accuracy: 0.6453 - loss: 1.0076 - val_accuracy: 0.5081 - val_loss: 0.9483\n",
      "Epoch 3/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 655ms/step - accuracy: 0.6337 - loss: 0.9425 - val_accuracy: 0.5081 - val_loss: 1.3501\n",
      "Epoch 4/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 651ms/step - accuracy: 0.6745 - loss: 0.7912 - val_accuracy: 0.5081 - val_loss: 1.6758\n",
      "Epoch 5/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 653ms/step - accuracy: 0.7270 - loss: 0.6510 - val_accuracy: 0.5081 - val_loss: 2.1206\n",
      "Epoch 6/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 651ms/step - accuracy: 0.7753 - loss: 0.4885 - val_accuracy: 0.5081 - val_loss: 2.6166\n",
      "Epoch 7/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 652ms/step - accuracy: 0.8087 - loss: 0.4329 - val_accuracy: 0.5081 - val_loss: 2.9597\n",
      "Epoch 8/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 653ms/step - accuracy: 0.8175 - loss: 0.4053 - val_accuracy: 0.5081 - val_loss: 3.0533\n",
      "Epoch 9/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 662ms/step - accuracy: 0.8633 - loss: 0.3064 - val_accuracy: 0.5081 - val_loss: 3.4491\n",
      "Epoch 10/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 653ms/step - accuracy: 0.8804 - loss: 0.2455 - val_accuracy: 0.5081 - val_loss: 3.5259\n",
      "Epoch 11/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 618ms/step - accuracy: 0.8923 - loss: 0.2472 - val_accuracy: 0.5081 - val_loss: 3.3549\n",
      "Epoch 12/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 612ms/step - accuracy: 0.9437 - loss: 0.1504 - val_accuracy: 0.5081 - val_loss: 3.7696\n",
      "Epoch 13/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 608ms/step - accuracy: 0.9694 - loss: 0.0940 - val_accuracy: 0.5081 - val_loss: 3.8619\n",
      "Epoch 14/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 612ms/step - accuracy: 0.9767 - loss: 0.0857 - val_accuracy: 0.5081 - val_loss: 4.3349\n",
      "Epoch 15/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 610ms/step - accuracy: 0.9622 - loss: 0.0900 - val_accuracy: 0.5081 - val_loss: 4.6711\n",
      "Epoch 16/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 607ms/step - accuracy: 0.9927 - loss: 0.0360 - val_accuracy: 0.5081 - val_loss: 4.7283\n",
      "Epoch 17/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 609ms/step - accuracy: 0.9867 - loss: 0.0435 - val_accuracy: 0.5081 - val_loss: 4.6337\n",
      "Epoch 18/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 647ms/step - accuracy: 0.9898 - loss: 0.0322 - val_accuracy: 0.5081 - val_loss: 3.8214\n",
      "Epoch 19/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 617ms/step - accuracy: 0.9917 - loss: 0.0366 - val_accuracy: 0.5081 - val_loss: 3.0462\n",
      "Epoch 20/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 608ms/step - accuracy: 0.9928 - loss: 0.0328 - val_accuracy: 0.5081 - val_loss: 2.8095\n",
      "Epoch 21/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 607ms/step - accuracy: 0.9937 - loss: 0.0270 - val_accuracy: 0.5285 - val_loss: 1.8519\n",
      "Epoch 22/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 609ms/step - accuracy: 0.9915 - loss: 0.0274 - val_accuracy: 0.5366 - val_loss: 1.8047\n",
      "Epoch 23/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 611ms/step - accuracy: 0.9936 - loss: 0.0202 - val_accuracy: 0.5813 - val_loss: 1.5907\n",
      "Epoch 24/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 607ms/step - accuracy: 0.9953 - loss: 0.0230 - val_accuracy: 0.5976 - val_loss: 1.6682\n",
      "Epoch 25/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 608ms/step - accuracy: 0.9980 - loss: 0.0162 - val_accuracy: 0.6504 - val_loss: 1.2224\n",
      "Epoch 26/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 611ms/step - accuracy: 0.9987 - loss: 0.0083 - val_accuracy: 0.7073 - val_loss: 0.8554\n",
      "Epoch 27/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 609ms/step - accuracy: 0.9972 - loss: 0.0094 - val_accuracy: 0.7520 - val_loss: 0.7498\n",
      "Epoch 28/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 607ms/step - accuracy: 0.9970 - loss: 0.0098 - val_accuracy: 0.7073 - val_loss: 1.0536\n",
      "Epoch 29/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 611ms/step - accuracy: 0.9910 - loss: 0.0201 - val_accuracy: 0.7195 - val_loss: 0.9994\n",
      "Epoch 30/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 609ms/step - accuracy: 0.9978 - loss: 0.0147 - val_accuracy: 0.7358 - val_loss: 1.1036\n",
      "Epoch 31/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 614ms/step - accuracy: 0.9927 - loss: 0.0168 - val_accuracy: 0.7480 - val_loss: 0.8901\n",
      "Epoch 32/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 608ms/step - accuracy: 0.9988 - loss: 0.0085 - val_accuracy: 0.7439 - val_loss: 0.9277\n",
      "Epoch 33/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 609ms/step - accuracy: 0.9943 - loss: 0.0104 - val_accuracy: 0.7724 - val_loss: 0.8325\n",
      "Epoch 34/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 611ms/step - accuracy: 0.9950 - loss: 0.0134 - val_accuracy: 0.7561 - val_loss: 0.8916\n",
      "Epoch 35/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 610ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.7561 - val_loss: 0.9447\n",
      "Epoch 36/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 611ms/step - accuracy: 0.9947 - loss: 0.0147 - val_accuracy: 0.7398 - val_loss: 0.8564\n",
      "Epoch 37/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 612ms/step - accuracy: 0.9997 - loss: 0.0078 - val_accuracy: 0.7398 - val_loss: 1.1078\n",
      "Epoch 38/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 611ms/step - accuracy: 0.9999 - loss: 0.0102 - val_accuracy: 0.6951 - val_loss: 1.3004\n",
      "Epoch 39/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 613ms/step - accuracy: 0.9965 - loss: 0.0148 - val_accuracy: 0.6992 - val_loss: 1.1958\n",
      "Epoch 40/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 609ms/step - accuracy: 0.9877 - loss: 0.0389 - val_accuracy: 0.7480 - val_loss: 0.9511\n",
      "Epoch 41/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 611ms/step - accuracy: 0.9851 - loss: 0.0341 - val_accuracy: 0.7276 - val_loss: 1.0257\n",
      "Epoch 42/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 606ms/step - accuracy: 0.9788 - loss: 0.0509 - val_accuracy: 0.7561 - val_loss: 0.8435\n",
      "Epoch 43/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 608ms/step - accuracy: 0.9803 - loss: 0.0491 - val_accuracy: 0.7276 - val_loss: 1.2190\n",
      "Epoch 44/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 610ms/step - accuracy: 0.9854 - loss: 0.0460 - val_accuracy: 0.7154 - val_loss: 1.3690\n",
      "Epoch 45/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 610ms/step - accuracy: 0.9882 - loss: 0.0429 - val_accuracy: 0.7683 - val_loss: 0.9212\n",
      "Epoch 46/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 609ms/step - accuracy: 0.9787 - loss: 0.0642 - val_accuracy: 0.7276 - val_loss: 1.1220\n",
      "Epoch 47/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 610ms/step - accuracy: 0.9841 - loss: 0.0538 - val_accuracy: 0.7195 - val_loss: 1.1920\n",
      "Epoch 48/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 608ms/step - accuracy: 0.9799 - loss: 0.0748 - val_accuracy: 0.7114 - val_loss: 1.2075\n",
      "Epoch 49/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 608ms/step - accuracy: 0.9859 - loss: 0.0473 - val_accuracy: 0.6911 - val_loss: 1.2759\n",
      "Epoch 50/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 608ms/step - accuracy: 0.9722 - loss: 0.0611 - val_accuracy: 0.7724 - val_loss: 0.9157\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.7921 - loss: 0.9346\n",
      "Test Accuracy: 0.7922077775001526\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_complex_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First convolutional block\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(128, 128, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Second convolutional block\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Third convolutional block\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Fourth convolutional block\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Flatten and dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_complex_model()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('complex_signature_verification_model123.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Difference: [[0.]]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image1(image_path):\n",
    "    processed_img = preprocess_image(image_path)\n",
    "    processed_img = processed_img / 255.0\n",
    "    processed_img = processed_img.reshape(1, 128, 128, 1)\n",
    "    return processed_img\n",
    "\n",
    "def predict_signature(model, real_signature_path, test_signature_path):\n",
    "    real_img = preprocess_image1(real_signature_path)\n",
    "    test_img = preprocess_image1(test_signature_path)\n",
    "    \n",
    "    real_pred = model.predict(real_img)\n",
    "    test_pred = model.predict(test_img)\n",
    "    \n",
    "    difference = np.abs(real_pred - test_pred)\n",
    "    return difference\n",
    "\n",
    "# Example usage\n",
    "real_signature_path = 'img1.jpeg'  # Provide a valid image path\n",
    "test_signature_path = 'img1.jpeg'  # Provide a valid image path\n",
    "difference = predict_signature(model, real_signature_path, test_signature_path)\n",
    "print(f\"Difference: {difference}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
